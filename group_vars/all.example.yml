# Server Helper Configuration - Main Variables
# This file contains all configuration options for Server Helper v1.0.0

---
# =============================================================================
# SYSTEM CONFIGURATION
# =============================================================================

# Base installation directory
base_install_dir: "/opt"

# Server hostname
hostname: "server-01"

# Timezone (use 'timedatectl list-timezones' to see available)
timezone: "America/New_York"

# Locale settings
locale: "en_US.UTF-8"

# =============================================================================
# MULTI-SERVER CONFIGURATION (Optional)
# =============================================================================
# When managing multiple physical servers, you can override these settings
# per-server in inventory/hosts.yml. See the secondary server example there.
#
# KEY CONSIDERATIONS FOR SECONDARY SERVERS:
#   1. Service ports: Ensure unique ports if servers share the same network
#      - Primary: Dockge 5001, Netdata 19999
#      - Secondary: Dockge 5002, Netdata 19998
#
#   2. Backup schedules: Stagger schedules to avoid resource contention
#      - Primary: "0 2 * * *" (2 AM)
#      - Secondary: "0 4 * * *" (4 AM)
#
#   3. Backup repositories: Use separate paths per server
#      - Primary: /mnt/nas/backup/server01
#      - Secondary: /mnt/nas/backup/server02
#
#   4. Hostnames: Set unique hostnames for each server
#      - Define in inventory/hosts.yml: hostname: "docker-server-02"
#
#   5. Monitoring: Central Uptime Kuma can monitor all servers
#      - Configure monitors pointing to each server's IP
#
# EXAMPLE inventory/hosts.yml override:
#   server02:
#     ansible_host: 192.168.1.101
#     hostname: "docker-server-02"
#     server_id: "02"
#     dockge:
#       port: 5002
#     monitoring:
#       netdata:
#         port: 19998
#     backups:
#       schedule: "0 4 * * *"
# =============================================================================

# =============================================================================
# USER ACCOUNT CONFIGURATION
# =============================================================================

system_users:
  # Create admin user account
  create_admin_user: false
  admin_user: "admin"
  admin_password: "{{ vault_system_users.admin_password }}"  # From vault
  admin_groups:
    - sudo
    - docker
  admin_passwordless_sudo: true
  admin_ssh_key: "{{ vault_system_users.admin_ssh_key }}"  # From vault (optional)

  # Disable root password login
  disable_root_password: true

  # Additional users (optional)
  additional_users: []
  # Example:
  # - name: "developer"
  #   password: "{{ vault_system_users.developer_password }}"
  #   groups: ["docker"]
  #   shell: "/bin/bash"
  #   ssh_key: "ssh-rsa AAAA..."

# =============================================================================
# VIRTUALIZATION CONFIGURATION
# =============================================================================

virtualization:
  # Install QEMU guest agent for Proxmox/KVM
  qemu_guest_agent: false

# =============================================================================
# LVM CONFIGURATION
# =============================================================================

lvm_config:
  enabled: false

  # Automatically extend Ubuntu default LV to use all available space
  auto_extend_ubuntu: true

  # Custom logical volumes to extend
  custom_lvs: []
  # Example:
  # - lv_path: "/dev/my-vg/my-lv"
  #   extend_percent: 100
  #   resize_cmd: "resize2fs"

  # New logical volumes to create
  create_lvs: []
  # Example:
  # - vg: "ubuntu-vg"
  #   lv: "docker-lv"
  #   size: "50G"
  #   fstype: "ext4"
  #   mount_point: "/var/lib/docker"
  #   mount_opts: "defaults,noatime"

# =============================================================================
# NAS CONFIGURATION (Optional)
# =============================================================================

nas:
  enabled: true

  # Multiple NAS shares supported
  shares:
    - ip: "192.168.1.100"
      share: "backup"
      mount: "/mnt/nas/backup"
      username: "{{ vault_nas_credentials[0].username }}"
      password: "{{ vault_nas_credentials[0].password }}"
      options: "vers=3.0,_netdev,nofail"
    
    # Add more shares as needed
    # - ip: "192.168.1.100"
    #   share: "media"
    #   mount: "/mnt/nas/media"
    #   username: "nasuser"
    #   password: "changeme"

# =============================================================================
# BACKUP CONFIGURATION (Restic)
# =============================================================================

restic:
  enabled: true
  
  # Cron schedule (cron syntax)
  schedule: "0 2 * * *"  # Daily at 2 AM
  
  # Retention policy
  retention:
    keep_daily: 7
    keep_weekly: 4
    keep_monthly: 6
    keep_yearly: 2
  
  # Backup destinations (enable any combination)
  destinations:
    # NAS backup
    nas:
      enabled: true
      path: "/mnt/nas/backup/restic"
      password: "{{ vault_restic_passwords.nas }}"

    # AWS S3 backup
    s3:
      enabled: false
      bucket: "my-server-backups"
      endpoint: "s3.amazonaws.com"
      region: "us-east-1"
      access_key: "{{ vault_aws_credentials.access_key }}"
      secret_key: "{{ vault_aws_credentials.secret_key }}"
      password: "{{ vault_restic_passwords.s3 }}"

    # Backblaze B2 backup
    b2:
      enabled: false
      bucket: "my-server-backups"
      account_id: "{{ vault_b2_credentials.account_id }}"
      account_key: "{{ vault_b2_credentials.account_key }}"
      password: "{{ vault_restic_passwords.b2 }}"

    # Local backup
    local:
      enabled: true
      path: "/opt/backups/restic"
      password: "{{ vault_restic_passwords.local }}"
  
  # Paths to backup
  backup_paths:
    - /opt/dockge/stacks
    - /opt/dockge/data
    - /etc
    - /home
    - /root
  
  # Paths to exclude
  exclude_patterns:
    - "*.tmp"
    - "*.log"
    - ".cache"
    - "__pycache__"
  
  # Uptime Kuma heartbeat (optional)
  uptime_kuma_push_url: ""  # e.g., "http://localhost:3001/api/push/BACKUP123"

# =============================================================================
# MONITORING CONFIGURATION
# =============================================================================

monitoring:
  # Automated remediation system
  auto_remediation:
    enabled: true

  # Webhook handler for triggering remediation
  remediation_webhook_port: 9090

  # Uptime Kuma push URLs for remediation notifications
  uptime_kuma_push_url: ""  # Main remediation push URL
  uptime_kuma_critical_key: ""  # Critical alerts push key
  uptime_kuma_service_key: ""   # Service alerts push key
  uptime_kuma_disk_key: ""      # Disk alerts push key
  uptime_kuma_cert_key: ""      # Certificate alerts push key

# Netdata - System and container metrics
netdata:
  enabled: true
  port: 19999

  # Netdata Cloud (optional)
  claim_token: ""  # Get from: https://app.netdata.cloud
  claim_rooms: ""  # Room IDs (comma-separated)

  # Alarms configuration
  alarms:
    enabled: true

    # CPU alarm thresholds
    cpu_warning: 80
    cpu_critical: 95

    # RAM alarm thresholds
    ram_warning: 80
    ram_critical: 95

    # Disk alarm thresholds
    disk_warning: 80
    disk_critical: 90

    # Health check interval (minutes)
    check_interval_minutes: 5

    # Uptime Kuma push URLs for alarms
    uptime_kuma_urls:
      cpu: ""      # http://localhost:3001/api/push/CPU123
      ram: ""      # http://localhost:3001/api/push/RAM123
      disk: ""     # http://localhost:3001/api/push/DISK123

# Uptime Kuma - Uptime monitoring and alerting
uptime_kuma:
  enabled: true
  port: 3001

  # Initial admin credentials (stored in vault)
  admin_username: "{{ vault_uptime_kuma_credentials.username }}"
  admin_password: "{{ vault_uptime_kuma_credentials.password }}"
  
  # Monitors (configured after initial setup)
  monitors:
    - name: "Netdata Health"
      type: "http"
      url: "http://localhost:19999/api/v1/info"
      interval: 60
      
    - name: "Dockge Health"
      type: "http"
      url: "http://localhost:5001"
      interval: 60
      
    - name: "Docker Daemon"
      type: "docker"
      docker_host: "unix:///var/run/docker.sock"
      interval: 60
  
  # Notification endpoints (configure in UI or via API)
  notifications: []
  # Example:
  # - type: "smtp"
  #   name: "Email Alerts"
  #   smtp_host: "smtp.gmail.com"
  #   smtp_port: 587
  #   smtp_username: "your-email@gmail.com"
  #   smtp_password: "your-app-password"

# =============================================================================
# LOGGING & VISUALIZATION (Loki + Promtail + Grafana)
# =============================================================================

logging:
  # Stack directory
  stack_dir: /opt/dockge/stacks/logging

  # Loki - Log aggregation server
  loki:
    enabled: true
    version: latest
    port: 3100
    retention_period: "744h"  # 31 days (720h = 30 days, 168h = 7 days)

  # Promtail - Log shipper (collects logs and sends to Loki)
  promtail:
    enabled: true
    version: latest

    # Additional custom log sources (optional)
    additional_jobs: []
    # Example:
    # - name: "nginx"
    #   path: "/var/log/nginx/*.log"
    #   pipeline_stages:
    #     - regex:
    #         expression: '^(?P<remote_addr>[\w\.]+) - .*'

  # Grafana - Visualization and dashboards
  grafana:
    enabled: true
    version: latest
    port: 3000
    admin_user: admin
    admin_password: "{{ vault_grafana_admin_password }}"

    # Grafana plugins to install (comma-separated)
    plugins: ""
    # Example: "grafana-clock-panel,grafana-piechart-panel"

    # SMTP configuration for email alerts (optional)
    smtp:
      enabled: false
      host: "smtp.gmail.com"
      port: 587
      user: ""
      from_address: "grafana@example.com"
      from_name: "Grafana"
      # SMTP password stored in vault: vault_grafana_smtp_password

# =============================================================================
# CONTAINER MANAGEMENT (Dockge)
# =============================================================================

dockge:
  enabled: true
  port: 5001
  stacks_dir: "/opt/dockge/stacks"
  data_dir: "/opt/dockge/data"

  # Admin credentials (stored in vault)
  admin_username: "admin"
  admin_password: "{{ vault_dockge_credentials.password }}"

# =============================================================================
# SECURITY CONFIGURATION
# =============================================================================

security:
  # fail2ban - Intrusion prevention
  fail2ban_enabled: true
  fail2ban_bantime: 3600  # seconds
  fail2ban_maxretry: 5
  
  # UFW Firewall
  ufw_enabled: true
  ufw_default_policy: "deny"
  ufw_allowed_ports:
    - 22      # SSH
    - 53      # DNS (Pi-hole - if enabled)
    - 5001    # Dockge
    - 8080    # Pi-hole Web UI (if enabled)
    - 19999   # Netdata
    - 3001    # Uptime Kuma
    - 3000    # Grafana / Semaphore (if enabled on different port, update accordingly)
    - 3100    # Loki (optional - only if accessing externally)
    - 9000    # Authentik (if enabled)
  
  # SSH Hardening
  ssh_hardening: true
  ssh_port: 22
  ssh_password_authentication: false
  ssh_permit_root_login: false
  ssh_max_auth_tries: 3
  ssh_client_alive_interval: 300
  
  # Lynis Security Auditing
  lynis_enabled: true
  lynis_schedule: "0 3 * * 0"  # Sunday at 3 AM
  lynis_uptime_kuma_push_url: ""  # Optional heartbeat
  
  # Automatic security updates
  unattended_upgrades: true
  auto_reboot: false
  auto_reboot_time: "03:00"

# =============================================================================
# ANSIBLE AUTOMATION (Semaphore UI)
# =============================================================================

semaphore:
  enabled: false
  port: 3000

  # Database configuration
  database:
    dialect: postgres  # Options: postgres, mysql, bolt (sqlite)
    host: semaphore-db
    port: 5432
    name: semaphore
    user: semaphore
    # Password stored in vault: vault_semaphore_db_password

  # Admin user configuration
  admin:
    username: admin
    email: admin@example.com
    # Password stored in vault: vault_semaphore_admin_password

  # Access key encryption secret stored in vault: vault_semaphore_access_key_encryption

  # Optional: Playbook path (if different from default)
  # playbook_path: /tmp/semaphore

  # Optional: Web host for external access
  # web_host: https://semaphore.example.com

  # Optional: LDAP authentication
  # ldap:
  #   enabled: true
  #   server: ldap://ldap.example.com:389
  #   binddn: "cn=admin,dc=example,dc=com"
  #   password: "{{ vault_semaphore_ldap_password }}"
  #   searchdn: "ou=users,dc=example,dc=com"

  # Optional: Email notifications (SMTP)
  # email:
  #   enabled: true
  #   sender: semaphore@example.com
  #   host: smtp.example.com
  #   port: 587
  #   username: semaphore@example.com
  #   password: "{{ vault_semaphore_smtp_password }}"

  # Optional: Telegram notifications
  # telegram:
  #   enabled: true
  #   token: "{{ vault_semaphore_telegram_token }}"
  #   chat: "{{ vault_semaphore_telegram_chat }}"

  # Optional: Slack notifications
  # slack:
  #   enabled: true
  #   url: "{{ vault_semaphore_slack_webhook }}"

# =============================================================================
# OPTIONAL SERVICES
# =============================================================================

# Watchtower - Automatic container updates
watchtower:
  enabled: false
  schedule: "0 4 * * *"  # Daily at 4 AM
  cleanup: true
  monitor_only: false  # Set to true for notifications without updates
  notifications:
    enabled: false
    url: ""  # Shoutrrr URL (e.g., discord://token@id)

# Reverse Proxy (Traefik)
reverse_proxy:
  enabled: false
  type: "traefik"

  # Domain configuration
  domain: "example.com"
  email: "admin@example.com"  # For Let's Encrypt notifications

  # Force HTTPS redirect
  force_https: true

  # SSL/TLS (now managed by certificates section above)
  ssl_enabled: true

  # Traefik-specific settings
  traefik:
    # Dashboard configuration
    dashboard_enabled: true
    dashboard_port: 8080
    dashboard_insecure: false  # Set true for HTTP access (not recommended)

    # Logging
    log_level: "INFO"  # DEBUG, INFO, WARN, ERROR

    # Entrypoints (automatically configured)
    entrypoints:
      - name: "web"
        port: 80
        redirect_to_https: true
      - name: "websecure"
        port: 443

# =============================================================================
# CERTIFICATE MANAGEMENT (Hybrid: Let's Encrypt + Smallstep CA)
# =============================================================================
#
# This configuration provides HYBRID certificate management:
#   - PUBLIC domains → Let's Encrypt (via DNS-01 challenge)
#   - INTERNAL domains → Smallstep CA (self-hosted, fully private)
#
# Privacy: Cloudflare is used in DNS-ONLY mode (no traffic proxying)
#          All HTTP/HTTPS traffic goes directly to your server
#
# Example:
#   mealie.tennogen.ca → Let's Encrypt certificate (public)
#   grafana.internal → Smallstep CA certificate (private)

certificates:
  # ============================================
  # PUBLIC CERTIFICATES (Let's Encrypt)
  # ============================================
  # For domains accessible from the internet
  public:
    enabled: true
    email: "admin@example.com"  # Let's Encrypt notifications

    # Challenge type for domain validation
    # Options: dns-01 (recommended), tls-alpn-01, http-01
    challenge: "dns-01"

    # DNS provider for DNS-01 challenge
    # Supported: cloudflare, route53, digitalocean, namecheap, godaddy
    dns_provider: "cloudflare"

    # Use Let's Encrypt staging for testing (avoids rate limits)
    staging: false

    # Domains to request certificates for
    # Wildcard domains require DNS-01 challenge
    domains:
      - "example.com"
      - "*.example.com"

    # AWS Route53 specific settings (if dns_provider: route53)
    # route53_zone_id: "ZXXXXXXXXXXXXX"

  # ============================================
  # INTERNAL CERTIFICATES (Smallstep CA)
  # ============================================
  # For internal/private domains (*.internal, *.local, etc.)
  internal:
    enabled: true
    email: "admin@internal"  # Internal CA notifications

    # Private domain suffix (services accessible as service.internal)
    domain: "internal"

# Smallstep CA - Self-Hosted Certificate Authority
# Provides ACME-compatible certificates for internal services
step_ca:
  enabled: true

  # CA identity
  name: "Server-Helper Internal CA"
  dns_names:
    - "step-ca"
    - "step-ca.internal"
    - "localhost"

  # Network configuration
  port: 9000

  # Certificate settings
  provisioner_name: "admin"
  default_cert_duration: "720h"    # 30 days
  max_cert_duration: "2160h"       # 90 days
  min_cert_duration: "5m"

  # ACME configuration (for Traefik integration)
  acme:
    enabled: true
    require_eab: false  # External Account Binding

  # SSH certificate support (optional)
  ssh:
    enabled: false

  # Data persistence
  data_dir: "/opt/step-ca"

  # Resource limits
  resources:
    memory: "128M"
    cpu: "0.5"

# =============================================================================
# SERVICE ROUTING (Public vs Internal)
# =============================================================================
# Define which services are public (Let's Encrypt) vs internal (Smallstep CA)
#
# Example configuration:
# services:
#   mealie:
#     enabled: true
#     public: true                    # Uses Let's Encrypt
#     domain: "mealie.tennogen.ca"
#     port: 9925
#     container: "mealie"
#
#   grafana:
#     enabled: true
#     public: false                   # Uses Smallstep CA
#     port: 3000
#     container: "grafana"
#     # Accessible at: grafana.internal

services: {}
# Add your services here. Examples:
#
# mealie:
#   enabled: true
#   public: true
#   domain: "mealie.tennogen.ca"
#   port: 9925
#   container: "mealie"
#   wildcard: false
#   rate_limit: true
#   compress: true
#   health_check: true
#   health_check_path: "/health"
#
# homepage:
#   enabled: true
#   public: true
#   domain: "home.tennogen.ca"
#   port: 3000
#   container: "homepage"
#
# vaultwarden:
#   enabled: true
#   public: true
#   domain: "vault.tennogen.ca"
#   port: 80
#   container: "vaultwarden"

# Authentik - Identity Provider & SSO
authentik:
  enabled: false
  version: "2024.12"
  http_port: 9000
  https_port: 9443

  # Database settings
  db_user: authentik
  db_name: authentik

  # Email configuration (optional - for password resets, invites)
  email:
    enabled: false
    host: "smtp.gmail.com"
    port: 587
    username: "your-email@gmail.com"
    from: "authentik@example.com"
    use_tls: true
    use_ssl: false

# =============================================================================
# DNS & SERVICE DISCOVERY (Pi-hole + Unbound)
# =============================================================================

dns:
  enabled: false

  # Stack directory
  stack_dir: /opt/dockge/stacks/dns

  # Network configuration
  network_name: dns
  private_domain: internal  # Services will be accessible as service.internal
  local_domain: local
  connect_to_monitoring: true  # Connect to monitoring network for Grafana/Netdata

  # Pi-hole - Network-wide ad blocking + local DNS
  pihole:
    version: latest
    port: 8080  # Web UI port (use 8080 to avoid conflict with port 80)
    https_port: false  # Set to 8443 if you want HTTPS
    theme: default-dark  # Options: default-light, default-dark, default-darker
    temp_unit: c  # c or f
    query_logging: true

    # Reverse DNS (conditional forwarding)
    rev_server: true
    rev_server_domain: local
    # rev_server_cidr and rev_server_target auto-detected from network

    # Traefik integration (if reverse proxy enabled)
    traefik_enabled: false
    virtual_host: ""  # e.g., pihole.example.com

  # Unbound - Recursive DNS resolver (privacy-focused)
  unbound:
    version: latest
    ipv6: false  # Enable if you use IPv6

    # Performance tuning
    num_threads: 2
    msg_cache_size: 50m
    rrset_cache_size: 100m

    # Upstream forwarding (set to true for faster resolution)
    forward_zone: false  # true = use upstream DNS, false = recursive from root
    forward_tls: true  # Use DNS-over-TLS when forwarding
    forward_servers:
      - 1.1.1.1  # Cloudflare
      - 1.0.0.1
      # - 9.9.9.9  # Quad9
      # - 149.112.112.112

  # Prometheus exporter for Grafana
  exporter:
    version: latest
    port: 9617
    interval: 30s

  # Monitoring integration
  monitoring:
    enabled: true
    grafana_dashboard: true
    uptime_kuma: true
    netdata: true

  # Custom DNS records (manually defined)
  custom_records: []
  # Example - add your custom domains here:
  # - domain: db.internal
  #   ip: 192.168.1.50
  # - domain: nas.internal
  #   ip: 192.168.1.100
  # - domain: router.internal
  #   ip: 192.168.1.1

  # Database services (auto-registered to DNS)
  database_services: []
  # Example:
  # - name: postgres
  #   ip: 192.168.1.50
  # - name: mysql
  #   ip: 192.168.1.51
  # - name: redis
  #   ip: 192.168.1.52

  # Application services (auto-registered to DNS)
  application_services: []
  # Example:
  # - name: webapp
  #   ip: 192.168.1.100
  # - name: api
  #   ip: 192.168.1.101

  # Additional private domains (won't query upstream)
  additional_private_domains: []
  # Example:
  # - home.arpa
  # - lan

# =============================================================================
# SELF-UPDATE CONFIGURATION (ansible-pull)
# =============================================================================

self_update:
  enabled: true
  schedule: "0 5 * * *"  # Daily at 5 AM
  
  # Git repository
  git_repo: "https://github.com/thelasttenno/Server-Helper.git"
  branch: "main"
  version: "v1.0.0"  # Git tag or branch
  
  # Playbook to run
  playbook: "playbooks/setup.yml"
  
  # Log file
  log_file: "/var/log/ansible-pull.log"
  
  # Only check for updates (don't apply)
  check_only: false

# =============================================================================
# DOCKER CONFIGURATION
# =============================================================================

docker:
  # Docker edition
  edition: "ce"  # ce or ee
  
  # Docker compose version
  compose_version: "2.23.0"
  
  # Docker daemon configuration
  daemon_config:
    log_driver: "json-file"
    log_opts:
      max_size: "10m"
      max_file: "3"
    storage_driver: "overlay2"
  
  # Docker networks
  networks:
    - name: "monitoring"
      driver: "bridge"
    - name: "proxy"
      driver: "bridge"
  
  # Docker volumes
  volumes: []

# =============================================================================
# NOTIFICATION SETTINGS
# =============================================================================

notifications:
  # Email notifications
  email:
    enabled: false
    smtp_host: "smtp.gmail.com"
    smtp_port: 587
    smtp_username: "your-email@gmail.com"
    smtp_password: "your-app-password"
    from_address: "server@example.com"
    to_addresses:
      - "admin@example.com"
  
  # Discord webhook
  discord:
    enabled: false
    webhook_url: ""
  
  # Telegram bot
  telegram:
    enabled: false
    bot_token: ""
    chat_id: ""

# =============================================================================
# CONTROL NODE CONFIGURATION (Centralized Services)
# =============================================================================
# These services run ONLY on the control node and aggregate data from all targets.
# Configure these when running: ansible-playbook playbooks/setup-control.yml

control_node_install_dir: "/opt/server-helper-control"

# Centralized Uptime Kuma - monitors all target servers
control_uptime_kuma:
  enabled: true
  version: "latest"
  port: 3001

# Centralized Grafana - dashboards for all servers
control_grafana:
  enabled: true
  version: "latest"
  port: 3000
  admin_user: "admin"
  admin_password: "{{ vault_control_grafana_password }}"
  plugins: ""  # Comma-separated list

  # SMTP for alerts (optional)
  smtp:
    enabled: false
    host: "smtp.gmail.com"
    port: 587
    user: ""
    from_address: ""
    from_name: "Grafana Control"

# Centralized Loki - aggregates logs from all target Promtail instances
control_loki:
  enabled: true
  version: "latest"
  port: 3100
  retention_period: "744h"  # 31 days

# Centralized Netdata Parent - aggregates metrics from all target Netdata children
control_netdata:
  enabled: true
  version: "latest"
  port: 19999
  # API key for child nodes to connect (stored in vault)
  stream_api_key: "{{ vault_netdata_stream_api_key }}"

# Scanopy (Trivy) - Container security scanning
control_scanopy:
  enabled: true
  trivy_version: "latest"
  port: 8080

# PruneMate - Automated Docker cleanup
control_prunemate:
  enabled: true
  schedule: "0 3 * * 0"  # Weekly Sunday 3 AM

# =============================================================================
# TARGET NODE STREAMING CONFIGURATION
# =============================================================================
# Configure target nodes to send data to the central control node.
# Set control_node_ip to your control node's IP address.

control_node_ip: ""  # e.g., "192.168.1.100"

# Target Promtail -> Central Loki
target_promtail:
  send_to_central: false  # Set true to send logs to central Loki
  # When true, Promtail on targets will push logs to:
  # http://{{ control_node_ip }}:{{ control_loki.port }}/loki/api/v1/push

# Target Netdata -> Central Netdata Parent
target_netdata:
  stream_to_parent: false  # Set true to stream metrics to parent
  # When true, Netdata children will stream to:
  # {{ control_node_ip }}:{{ control_netdata.port }}
  # Using API key: {{ control_netdata.stream_api_key }}

# =============================================================================
# ADVANCED SETTINGS
# =============================================================================

# Performance
performance:
  max_parallel_tasks: 10
  connection_timeout: 30

# Feature flags
features:
  experimental: false
  debug_mode: false
